{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132ce20b-b291-45a6-963d-11bfcfbd0567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 12:40:30.695069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c589b44b-85a1-4a4a-a1c4-6f157e741796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Einlesen einer Datei\n",
    "def load_dat(path, input_end, crop_start, crop_stop):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            # Zeilen aus der Datei lesen und in eine Liste von Zeichenketten aufteilen\n",
    "            lines = file.read().splitlines()\n",
    "\n",
    "            # Die Daten in eine Matrix umwandeln\n",
    "            data = [list(map(float, line.split()[:input_end])) for line in lines]\n",
    "            data = np.array(data)\n",
    "\n",
    "            # Format ermitteln und ggf. transponieren\n",
    "            size = (len(data), len(data[0]))\n",
    "            if (size[0] > 52):\n",
    "                      data = np.transpose(data)\n",
    "            \n",
    "            data = data[crop_start:crop_stop]\n",
    "\n",
    "        # Größe der neuen Matrix ausgeben\n",
    "        #new_size = (len(data), len(data[0]))\n",
    "        #print(f\"Größe der Trainingsdaten Matrix: {new_size}\")\n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'Die Datei {path} wurde nicht gefunden.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Fehler beim Lesen der Datei: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df49673b-32e7-4a87-9ac7-3bb2e1025fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELPATH = 'Dataset\\\\'\n",
    "RELPATH = '/home/ki-projekt/KI-Projekt/data/TennesseEastman/'\n",
    "\n",
    "data_raw = []\n",
    "dataTE_raw = []\n",
    "\n",
    "# Dateien einlesen\n",
    "for datei in sorted(os.listdir(RELPATH)):\n",
    "    filePath = RELPATH + datei\n",
    "\n",
    "    #print('Look, i found a file: ' + filePath)\n",
    "\n",
    "    if 'te' in datei:\n",
    "        data_matrix = load_dat(filePath, 480, 0, 480)\n",
    "        dataTE_raw.append(data_matrix)\n",
    "    else:\n",
    "        data_matrix = load_dat(filePath, 480, 0, 480)\n",
    "        data_raw.append(data_matrix)\n",
    "\n",
    "\n",
    "data_raw = np.array(data_raw)\n",
    "dataTE_raw = np.array(dataTE_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81881ff0-8eff-4a40-8a58-0823c9b7e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normieren\n",
    "data_norm = []\n",
    "dataTE_norm = []\n",
    "\n",
    "for i in range(22):\n",
    "    array2ndD = []\n",
    "    array2ndDTE = []\n",
    "\n",
    "    for k in range(52):\n",
    "        normOfSensorData = np.linalg.norm(data_raw[i][k])\n",
    "        normOfSensorDataTE = np.linalg.norm(dataTE_raw[i][k])\n",
    "        meanOfSensorData = np.mean(data_raw[i][k])\n",
    "        meanOfSensorDataTE = np.mean(dataTE_raw[i][k])\n",
    "        \n",
    "        array3rdD = []\n",
    "        array3rdDTE = []\n",
    "            \n",
    "        for m in range(480):\n",
    "            array3rdD.append((data_raw[i][k][m] - meanOfSensorData)/normOfSensorData)\n",
    "            array3rdDTE.append((dataTE_raw[i][k][m] - meanOfSensorDataTE)/normOfSensorDataTE) ### clean up\n",
    "\n",
    "        array2ndD.append(array3rdD)\n",
    "        array2ndDTE.append(array3rdDTE)\n",
    "\n",
    "    data_norm.append(array2ndD)\n",
    "    dataTE_norm.append(array2ndDTE)\n",
    "\n",
    "data_norm = np.array(data_norm)\n",
    "dataTE_norm = np.array(dataTE_norm)\n",
    "\n",
    "\n",
    "# Debug \n",
    "# print(np.max(data_norm))\n",
    "# print(np.min(data_norm))\n",
    "#print(data_norm)\n",
    "#print(dataTE_norm.shape)\n",
    "\n",
    "#print(data_raw[1][1].shape)\n",
    "#print(data_raw[1][1])\n",
    "#print(np.linalg.norm(data_raw[1], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58e49dd-fae3-4e5c-9d88-c3d2818707f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aus normierter Matrix Trainigsdaten erstellen\n",
    "\n",
    "training_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(22):\n",
    "      # hier label erzeugen -> one-hot vector\n",
    "      oneHotVector = np.eye(22)[i, :]\n",
    "      fault_data = np.transpose(data_norm[i])\n",
    "      fault_dataTE = np.transpose(dataTE_norm[i])\n",
    "\n",
    "      for k in range(480):\n",
    "            training_data.append([fault_data[k], oneHotVector])\n",
    "            test_data.append([fault_dataTE[k], oneHotVector])\n",
    "\n",
    "# durchmischen\n",
    "\n",
    "random.shuffle(training_data) # anpassen mit tf-Funktion mit batch-size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc73d61f-22d7-4b28-9674-06e1a5ea45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training data\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for testvalues, label in training_data:\n",
    "      x_train.append(testvalues)\n",
    "      y_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ca36e2-6fc8-4e63-b34c-40fb360bdf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test data\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for testvalues, label in test_data:\n",
    "      x_test.append(testvalues)\n",
    "      y_test.append(label)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9a36c-945a-425f-a535-9d93229ac2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e3c7b-6e41-4b97-bdaf-c8b242015126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51918e70-45b2-4df3-b036-e6a95b387ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(x_train.shape[1], 1), return_sequences=True))\n",
    "\n",
    "for _ in range(20):  # LSTM-Schichten hinzufügen\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "model.add(Dense(22, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Modell trainieren\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Modellbewertung\n",
    "train_accuracy = model.evaluate(x_train, y_train, verbose=0)[1]\n",
    "test_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "y_true = [i.argmax() for i in y_test]\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7e948-c6ba-4851-a696-6933fbc25c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99838c8d-568d-44c1-99ea-860f9e4d92aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
